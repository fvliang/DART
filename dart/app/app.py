import argparse
import html
import os
import time

import gradio as gr
import torch

from dart.model.dart_model import DartModel
from dart.model.template import TEMPLATE_REGISTRY
from third_party.eagle.eagle.model.ea_model import EaModel


def parse_args():
    parser = argparse.ArgumentParser(description="DART Gradio Demo")
    parser.add_argument(
        "--base-model-name-or-path",
        default="Qwen/Qwen3-4B",
        help="Path or repo id of the base LLM",
    )
    parser.add_argument(
        "--dart-model-name-or-path",
        default="fvliang/qwen4b-dart",
        help="Path or repo id of the DART draft model",
    )
    parser.add_argument(
        "--ngram-model-name-or-path",
        default="fvliang/dart-qwen3-ngram",
        # default=None,
        help="Path or repo id of the ngram model for DART tree search",
    )
    parser.add_argument(
        "--use-small-ngram",
        action="store_true",
        help="If use small ngram model",
    )
    parser.add_argument(
        "--template-name",
        default="qwen",
        choices=TEMPLATE_REGISTRY.get_all_template_names(),
        help="Template name used to format chat messages",
    )
    parser.add_argument(
        "--device",
        default=None,
        help="Target device, e.g. cpu / cuda / cuda:0. Default: auto-detect",
    )
    parser.add_argument(
        "--max-new-tokens",
        type=int,
        default=512,
        help="Maximum number of new tokens generated by DART",
    )
    parser.add_argument(
        "--max-length",
        type=int,
        default=1024,
        help="Maximum number of new tokens generated by DART",
    )
    parser.add_argument(
        "--share",
        action="store_true",
        help="Launch Gradio with share link",
    )
    parser.add_argument(
        "--compare-eagle3",
        action="store_true",
        help="If compare with Eagle3 model",
    )
    parser.add_argument(
        "--eagle3-model-name-or-path",
        default="AngelSlim/Qwen3-4B_eagle3",
        help="Path or repo id of the eagle3 LLM",
    )
    parser.add_argument(
        "--listen",
        action="store_true",
        help="Listen on 0.0.0.0 for remote access",
    )
    parser.add_argument(
        "--server-port",
        type=int,
        default=40003,
        help="Custom Gradio server port",
    )
    return parser.parse_args()


def select_device(user_device: str | None) -> torch.device:
    if user_device:
        device = torch.device(user_device)
        if device.type == "cuda":
            if not torch.cuda.is_available():
                raise ValueError("CUDA device requested but no GPU is available.")
            # After CUDA_VISIBLE_DEVICES is set, device indices are remapped
            # So cuda:0 refers to the first visible device
            if device.index is not None and device.index >= torch.cuda.device_count():
                raise ValueError(
                    f"Device index {device.index} is out of range. "
                    f"Only {torch.cuda.device_count()} device(s) available "
                    f"(after CUDA_VISIBLE_DEVICES filtering)."
                )
        return device
    return torch.device("cuda" if torch.cuda.is_available() else "cpu")


def build_prompt(pure_history, tokenizer, template_name: str) -> str:
    template = TEMPLATE_REGISTRY.get(template_name)
    messages = []
    if template.system_prompt:
        messages.append({"role": "system", "content": template.system_prompt})
    for query, resp in pure_history:
        messages.append({"role": "user", "content": query})
        if resp is not None:
            messages.append({"role": "assistant", "content": resp})
    return tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=False,
    )


def warmup(model: DartModel, template_name: str, device: torch.device) -> None:
    """Run at least two short DART generations to warm up kernels and caches."""
    warmup_histories = [
        [["Hello!", None]],
        [["Say hi in one short sentence.", None]],
    ]
    print("Running DART warmup (2 rounds)...")
    start = time.time()
    for pure_history in warmup_histories:
        prompt = build_prompt(pure_history, model.tokenizer, template_name)
        input_ids = model.tokenizer(
            prompt, return_tensors="pt", add_special_tokens=False
        ).input_ids.to(device)
        # dart_generate returns the full output tensor (not a generator)
        _ = model.dart_generate(
            input_ids,
            max_new_token_num=8,
            max_length=max(args.max_length, 4096),
            remain_total=60,
        )
    elapsed = time.time() - start
    print(f"DART warmup completed in {elapsed:.2f}s.")


def user(user_message, history, session_state):
    history = history or []
    pure_history = session_state.get("pure_history", [])
    pure_history.append([user_message, None])
    session_state["pure_history"] = pure_history
    return "", history + [[user_message, None]], session_state


def regenerate(history, session_state):
    if not history:
        return history, None, "0.00 tokens/s", "0.00", session_state
    pure_history = session_state.get("pure_history", [])
    pure_history[-1][-1] = None
    session_state["pure_history"] = pure_history
    if len(history) > 1:
        new_history = history[:-1]
        last_user_message = history[-1][0]
        return (
            new_history + [[last_user_message, None]],
            None,
            "0.00 tokens/s",
            "0.00",
            session_state,
        )
    history[-1][1] = None
    return history, None, "0.00 tokens/s", "0.00", session_state


def clear(history, session_state):
    session_state["pure_history"] = []
    return [], "0.00 tokens/s", "0.00", session_state


def truncate_list(lst, num):
    if num not in lst:
        return lst
    first_index = lst.index(num)
    return lst[: first_index + 1]


def find_list_markers(text):
    import re

    pattern = re.compile(r"(?m)(^\d+\.\s|\n)")
    matches = pattern.finditer(text)
    return [(match.start(), match.end()) for match in matches]


def checkin(pointer, start, marker):
    for b, e in marker:
        if b <= pointer < e:
            return True
        if b <= start < e:
            return True
    return False


def format_text_with_highlight(token_ids, tokenizer):
    """
    Â∞Ü token_ids Ê†ºÂºèÂåñ‰∏∫Â∏¶È´ò‰∫ÆÁöÑ HTML ÊñáÊú¨
    Á¨¨‰∏Ä‰∏™ token ÊòØÂéüÊ®°Âûã‰∫ßÁîüÁöÑÔºå‰ªéÁ¨¨‰∫å‰∏™ÂºÄÂßãÊòØ draft model ‰∫ßÁîüÁöÑÔºàÈúÄË¶ÅÈ´ò‰∫ÆÔºâ
    
    Args:
        token_ids: torch.Tensor, Êñ∞ÁîüÊàêÁöÑ token IDs
        tokenizer: tokenizer ÂØπË±°
    
    Returns:
        tuple: (formatted_html_text, plain_text)
    """
    if len(token_ids) == 0:
        return "", ""
    
    # Ëß£Á†ÅÊâÄÊúâ token
    all_tokens_text = tokenizer.decode(
        token_ids,
        skip_special_tokens=True,
        spaces_between_special_tokens=False,
        clean_up_tokenization_spaces=True,
    )
    
    if len(token_ids) == 1:
        # Âè™Êúâ‰∏Ä‰∏™ tokenÔºåÊòØÂéüÊ®°Âûã‰∫ßÁîüÁöÑÔºå‰∏çÈúÄË¶ÅÈ´ò‰∫Æ
        return all_tokens_text, all_tokens_text
    
    # Á¨¨‰∏Ä‰∏™ token ÊòØÂéüÊ®°Âûã‰∫ßÁîüÁöÑ
    first_token_text = tokenizer.decode(
        token_ids[0:1],
        skip_special_tokens=True,
        spaces_between_special_tokens=False,
        clean_up_tokenization_spaces=True,
    )
    
    # ‰ªéÁ¨¨‰∫å‰∏™ token ÂºÄÂßãÊòØ draft model ‰∫ßÁîüÁöÑ
    draft_tokens_text = tokenizer.decode(
        token_ids[1:],
        skip_special_tokens=True,
        spaces_between_special_tokens=False,
        clean_up_tokenization_spaces=True,
    )
    
    # ËΩ¨‰πâ HTML ÁâπÊÆäÂ≠óÁ¨¶‰ª•ÈÅøÂÖçÂÆâÂÖ®ÈóÆÈ¢ò
    first_token_text_escaped = html.escape(first_token_text)
    draft_tokens_text_escaped = html.escape(draft_tokens_text)
    
    # ÊûÑÂª∫Â∏¶È´ò‰∫ÆÁöÑ HTML ÊñáÊú¨
    # ‰ΩøÁî® <mark> Ê†áÁ≠æÈ´ò‰∫ÆÊòæÁ§∫ draft model ‰∫ßÁîüÁöÑ token
    formatted_html = (
        first_token_text_escaped
        + '<span style="color: orange;">'
        + draft_tokens_text_escaped
        + '</span>'
    )
    
    return formatted_html, all_tokens_text


def bot(history, temperature, top_p, top_k, remain_total, model_type, session_state):
    if not history:
        return history, "0.00 tokens/s", "0.00", session_state
    pure_history = session_state.get("pure_history", [])
    prompt = build_prompt(pure_history, model.tokenizer, args.template_name)
    input_ids = model.tokenizer(
        prompt, return_tensors="pt", add_special_tokens=False
    ).input_ids.to(device)
    start_time = time.time()
    forward_calls = 0
    total_tokens = 0

    if model_type == "DART":
        # Use DART model generation
        accumulated_text = ""
        accumulated_html_text = ""
        for output in model._dart_generate(
            input_ids,
            temperature=temperature,
            top_p=top_p,
            top_k=int(top_k),
            max_new_token_num=args.max_new_tokens,
            max_length=args.max_length,
            remain_total=int(remain_total),
        ):
            forward_calls += 1
            # Ê†ºÂºèÂåñÂ∏¶È´ò‰∫ÆÁöÑÊñáÊú¨
            formatted_html, plain_text = format_text_with_highlight(
                output["id"],
                model.tokenizer
            )
            accumulated_html_text += formatted_html
            accumulated_text += plain_text
            # For baseline model, each forward call generates one token
            total_tokens += output["accept_length"] + 1
            # ‰ΩøÁî® HTML Ê†ºÂºèÁöÑÊñáÊú¨ÊòæÁ§∫Âú® UI ‰∏≠
            history[-1][1] = accumulated_html_text
            # pure_history ‰∏≠‰øùÂ≠òÁ∫ØÊñáÊú¨ÔºàÁî®‰∫éÂêéÁª≠ÁöÑ prompt ÊûÑÂª∫Ôºâ
            pure_history[-1][1] = accumulated_text
            session_state["pure_history"] = pure_history
            elapsed = max(time.time() - start_time, 1e-6)
            speed = total_tokens / elapsed
            compression = total_tokens / forward_calls if forward_calls else 0.0
            yield history, f"{speed:.2f} tokens/s", f"{compression:.2f}", session_state
    elif model_type == "Eagle3" and eagle_model is not None:
        accumulated_text = ""
        accumulated_html_text = ""
        #  naive_generate
        for output in eagle_model.ea_generate(
            input_ids,
            temperature=temperature,
            top_p=top_p,
            top_k=int(top_k),
            max_new_tokens=args.max_new_tokens,
            max_length=args.max_length,
        ):
            forward_calls += 1
            # Ê†ºÂºèÂåñÂ∏¶È´ò‰∫ÆÁöÑÊñáÊú¨
            formatted_html, plain_text = format_text_with_highlight(
                output["id"],
                model.tokenizer
            )
            accumulated_html_text += formatted_html
            accumulated_text += plain_text
            # For baseline model, each forward call generates one token
            total_tokens += output["accept_length"] + 1
            # ‰ΩøÁî® HTML Ê†ºÂºèÁöÑÊñáÊú¨ÊòæÁ§∫Âú® UI ‰∏≠
            history[-1][1] = accumulated_html_text
            # pure_history ‰∏≠‰øùÂ≠òÁ∫ØÊñáÊú¨ÔºàÁî®‰∫éÂêéÁª≠ÁöÑ prompt ÊûÑÂª∫Ôºâ
            pure_history[-1][1] = accumulated_text
            session_state["pure_history"] = pure_history
            elapsed = max(time.time() - start_time, 1e-6)
            speed = total_tokens / elapsed
            compression = total_tokens / forward_calls if forward_calls else 0.0
            yield history, f"{speed:.2f} tokens/s", f"{compression:.2f}", session_state
    else:
        # Use baseline (AR) model generation
        accumulated_text = ""
        for output in model._ar_generate(
            input_ids,
            temperature=temperature,
            top_p=top_p,
            top_k=int(top_k),
            max_new_token_num=args.max_new_tokens,
            max_length=args.max_length
        ):
            forward_calls += 1
            cur_text = model.tokenizer.decode(
                output["id"],
                skip_special_tokens=True,
                spaces_between_special_tokens=False,
                clean_up_tokenization_spaces=True,
            )
            # Accumulate text for baseline model
            accumulated_text += cur_text
            # For baseline model, each forward call generates one token
            total_tokens += 1
            history[-1][1] = accumulated_text
            pure_history[-1][1] = accumulated_text
            session_state["pure_history"] = pure_history
            elapsed = max(time.time() - start_time, 1e-6)
            speed = total_tokens / elapsed
            compression = total_tokens / forward_calls if forward_calls else 0.0
            yield history, f"{speed:.2f} tokens/s", f"{compression:.2f}", session_state


def build_demo():
    custom_css = """
    /* Êï¥‰ΩìËÉåÊôØÂíåÂ≠ó‰Ωì */
    .gradio-container {
        font-family: 'Inter', 'Segoe UI', sans-serif !important;
        background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%) !important;
        min-height: 100vh;
        max-width: 100% !important;
        padding-left: 2% !important;
        padding-right: 2% !important;
    }
    
    /* ÁßªÈô§ÈªòËÆ§ÁöÑÊúÄÂ§ßÂÆΩÂ∫¶ÈôêÂà∂ */
    .contain {
        max-width: 100% !important;
    }
    
    /* ÂéªÈô§ÊâÄÊúâÈªòËÆ§ÁÅ∞Ëâ≤ËÉåÊôØ */
    .gr-group, .gr-box, .gr-form, .gr-panel, .gr-block {
        background: transparent !important;
    }
    
    footer {
        display: none !important;
    }
    
    /* Âè≥‰æßËÅäÂ§©Âå∫ÂüüÂ°´Êª°È´òÂ∫¶ */
    #chat-column {
        display: flex !important;
        flex-direction: column !important;
        height: calc(100vh - 180px) !important;
    }
    
    #chat-column > div {
        flex: 1 !important;
        display: flex !important;
        flex-direction: column !important;
    }
    
    /* ‰∏ªÊ†áÈ¢òÊ†∑Âºè */
    #main-title {
        text-align: center;
        background: linear-gradient(90deg, #00d4ff, #7b2cbf, #ff006e);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
        font-size: 2.5em !important;
        font-weight: 700;
        margin-bottom: 0.5em;
        text-shadow: 0 0 30px rgba(0, 212, 255, 0.3);
    }
    
    /* ÂâØÊ†áÈ¢ò */
    #sub-title {
        text-align: center;
        color: #a0a0a0 !important;
        font-size: 1em !important;
        margin-bottom: 1.5em;
    }
    
    /* Â∑¶‰æßÊéßÂà∂Èù¢Êùø */
    #control-panel {
        background: rgba(255, 255, 255, 0.03) !important;
        border: 1px solid rgba(255, 255, 255, 0.1) !important;
        border-radius: 16px !important;
        padding: 20px !important;
        backdrop-filter: blur(10px);
    }
    
    /* Èù¢ÊùøÊ†áÈ¢ò */
    .panel-title {
        color: #00d4ff !important;
        font-size: 1.1em !important;
        font-weight: 600 !important;
        margin-bottom: 0.8em !important;
        padding-bottom: 0.5em;
        border-bottom: 1px solid rgba(0, 212, 255, 0.3);
    }
    
    /* È°∂ÈÉ®‰ª™Ë°®ÁõòÂå∫Âüü */
    #dashboard {
        background: rgba(255, 255, 255, 0.03) !important;
        border: 1px solid rgba(255, 255, 255, 0.1) !important;
        border-radius: 20px !important;
        padding: 20px 40px !important;
        margin-bottom: 20px !important;
        backdrop-filter: blur(10px);
    }
    
    /* ÈÄüÂ∫¶Âç°Áâá */
    #speed-card {
        background: linear-gradient(135deg, rgba(0, 212, 255, 0.15) 0%, rgba(0, 212, 255, 0.05) 100%) !important;
        border: 2px solid rgba(0, 212, 255, 0.4) !important;
        border-radius: 16px !important;
        padding: 15px 25px !important;
        text-align: center;
    }
    
    #speed-card textarea {
        color: #00d4ff !important;
        font-size: 42px !important;
        font-weight: 800 !important;
        text-align: center !important;
        background: transparent !important;
        border: none !important;
        text-shadow: 0 0 20px rgba(0, 212, 255, 0.5);
        line-height: 1.2 !important;
    }
    
    /* ÂéãÁº©ÁéáÂç°Áâá */
    #compression-card {
        background: linear-gradient(135deg, rgba(0, 255, 136, 0.15) 0%, rgba(0, 255, 136, 0.05) 100%) !important;
        border: 2px solid rgba(0, 255, 136, 0.4) !important;
        border-radius: 16px !important;
        padding: 15px 25px !important;
        text-align: center;
    }
    
    #compression-card textarea {
        color: #00ff88 !important;
        font-size: 42px !important;
        font-weight: 800 !important;
        text-align: center !important;
        background: transparent !important;
        border: none !important;
        text-shadow: 0 0 20px rgba(0, 255, 136, 0.5);
        line-height: 1.2 !important;
    }
    
    /* ‰ª™Ë°®ÁõòÊ†áÁ≠æ */
    #speed-card label, #compression-card label {
        font-size: 14px !important;
        text-transform: uppercase !important;
        letter-spacing: 2px !important;
        opacity: 0.8;
    }
    
    /* ÊªëÂùóÊ†∑Âºè */
    .gr-slider input[type="range"] {
        accent-color: #7b2cbf !important;
    }
    
    /* ËÅäÂ§©Âå∫Âüü */
    #chatbot {
        background: rgba(255, 255, 255, 0.02) !important;
        border: 1px solid rgba(255, 255, 255, 0.1) !important;
        border-radius: 16px !important;
        box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3) !important;
        flex: 1 !important;
        min-height: 500px !important;
        height: calc(100vh - 320px) !important;
    }
    
    #chatbot .message {
        border-radius: 12px !important;
        padding: 12px 16px !important;
    }
    
    /* ËæìÂÖ•Ê°Ü */
    #msg-input textarea {
        background: rgba(255, 255, 255, 0.05) !important;
        border: 1px solid rgba(255, 255, 255, 0.2) !important;
        border-radius: 12px !important;
        color: white !important;
        font-size: 15px !important;
        transition: all 0.3s ease !important;
    }
    
    #msg-input textarea:focus {
        border-color: #00d4ff !important;
        box-shadow: 0 0 15px rgba(0, 212, 255, 0.2) !important;
    }
    
    /* ÊåâÈíÆÊ†∑Âºè */
    .primary-btn {
        background: linear-gradient(135deg, #00d4ff 0%, #7b2cbf 100%) !important;
        border: none !important;
        border-radius: 10px !important;
        color: white !important;
        font-weight: 600 !important;
        padding: 10px 20px !important;
        transition: all 0.3s ease !important;
        box-shadow: 0 4px 15px rgba(0, 212, 255, 0.3) !important;
    }
    
    .primary-btn:hover {
        transform: translateY(-2px) !important;
        box-shadow: 0 6px 20px rgba(0, 212, 255, 0.4) !important;
    }
    
    .secondary-btn {
        background: rgba(255, 255, 255, 0.1) !important;
        border: 1px solid rgba(255, 255, 255, 0.2) !important;
        border-radius: 10px !important;
        color: white !important;
        font-weight: 500 !important;
        transition: all 0.3s ease !important;
    }
    
    .secondary-btn:hover {
        background: rgba(255, 255, 255, 0.15) !important;
        border-color: rgba(255, 255, 255, 0.3) !important;
    }
    
    .danger-btn {
        background: rgba(255, 0, 110, 0.2) !important;
        border: 1px solid rgba(255, 0, 110, 0.4) !important;
        border-radius: 10px !important;
        color: #ff006e !important;
        font-weight: 500 !important;
        transition: all 0.3s ease !important;
    }
    
    .danger-btn:hover {
        background: rgba(255, 0, 110, 0.3) !important;
    }
    
    /* RadioÊåâÈíÆÁªÑ */
    .gr-radio {
        background: rgba(255, 255, 255, 0.05) !important;
        border-radius: 10px !important;
        padding: 8px !important;
    }
    
    /* ÊèêÁ§∫ÊñáÂ≠ó */
    .hint-text {
        color: #888 !important;
        font-size: 0.85em !important;
        font-style: italic;
        padding: 8px 12px;
        background: rgba(255, 255, 255, 0.03);
        border-radius: 8px;
        border-left: 3px solid #7b2cbf;
    }
    
    /* ÂàÜÈöîÁ∫ø */
    .divider {
        height: 1px;
        background: linear-gradient(90deg, transparent, rgba(255,255,255,0.1), transparent);
        margin: 15px 0;
    }
    """
    
    with gr.Blocks(css=custom_css, theme=gr.themes.Base(
        primary_hue="cyan",
        secondary_hue="purple",
        neutral_hue="slate",
    ).set(
        body_background_fill="transparent",
        block_background_fill="transparent",
        block_label_text_color="white",
        block_title_text_color="white",
        input_background_fill="rgba(255,255,255,0.05)",
        button_primary_background_fill="linear-gradient(135deg, #00d4ff 0%, #7b2cbf 100%)",
    )) as demo:
        state = gr.State({"pure_history": []})
        
        # Ê†áÈ¢òÂå∫Âüü
        gr.Markdown("# üöÄ DART Chatbot", elem_id="main-title")
        gr.Markdown("*Dynamic Adaptive Reasoning Transformer - Fast & Efficient Generation*", elem_id="sub-title")
        
        # È°∂ÈÉ®‰ª™Ë°®Áõò - ÊòæÁ§∫ÂÖ≥ÈîÆÊÄßËÉΩÊåáÊ†á
        with gr.Group(elem_id="dashboard"):
            with gr.Row():
                with gr.Column(scale=1, elem_id="speed-card"):
                    speed_box = gr.Textbox(
                        label="‚ö° SPEED",
                        interactive=False,
                        value="0.00 tokens/s",
                        show_label=True
                    )
                with gr.Column(scale=1, elem_id="compression-card"):
                    compression_box = gr.Textbox(
                        label="üìä COMPRESSION RATIO",
                        interactive=False,
                        value="0.00√ó",
                        show_label=True
                    )
        
        # ‰∏ªÂ∏ÉÂ±ÄÔºöÂ∑¶‰æßÊéßÂà∂Èù¢Êùø + Âè≥‰æßËÅäÂ§©Âå∫
        with gr.Row():
            # Â∑¶‰æßÊéßÂà∂Èù¢Êùø
            with gr.Column(scale=1, min_width=320):
                with gr.Group(elem_id="control-panel"):
                    gr.Markdown("### üéØ Model Selection", elem_classes="panel-title")
                    model_choices = ["DART", "Baseline"]
                    if args.compare_eagle3:
                        model_choices.append("Eagle3")
                    model_type = gr.Radio(
                        choices=model_choices,
                        value="DART",
                        label="Model Type",
                        info="Choose generation model",
                        elem_classes="gr-radio"
                    )
                    
                    gr.HTML('<div class="divider"></div>')
                    
                    gr.Markdown("### üéõÔ∏è Generation Settings", elem_classes="panel-title")
                    temperature = gr.Slider(
                        minimum=0.0, maximum=1.5, step=0.01,
                        label="üå°Ô∏è Temperature",
                        value=0.0,
                        info="Controls randomness (0=deterministic)"
                    )
                    top_p = gr.Slider(
                        minimum=0.0, maximum=1.0, step=0.01,
                        label="üé≤ Top-P",
                        value=0.9,
                        info="Nucleus sampling threshold"
                    )
                    top_k = gr.Slider(
                        minimum=0, maximum=200, step=1,
                        label="üìà Top-K",
                        value=0,
                        info="Top-K filtering (0=disabled)"
                    )
                    
                    gr.HTML('<div class="divider"></div>')
                    
                    gr.Markdown("### üå≥ Tree Search Settings", elem_classes="panel-title")
                    remain_total = gr.Slider(
                        minimum=1, maximum=256, step=1,
                        label="üåø Remain Total",
                        value=60,
                        info="Total remaining tokens"
                    )
                    
                    gr.Markdown(
                        "üí° **Compression Ratio** = Generated Tokens √∑ Forward Passes. Higher is better!",
                        elem_classes="hint-text"
                    )
            
            # Âè≥‰æßËÅäÂ§©Âå∫Âüü
            with gr.Column(scale=3, min_width=600, elem_id="chat-column"):
                chatbot = gr.Chatbot(
                    height=None,
                    show_label=False,
                    sanitize_html=False,
                    elem_id="chatbot",
                    bubble_full_width=False,
                    avatar_images=(None, "https://em-content.zobj.net/source/twitter/376/robot_1f916.png")
                )
                
                with gr.Group():
                    msg = gr.Textbox(
                        label="",
                        placeholder="‚ú® Type your message here and press Enter...",
                        show_label=False,
                        elem_id="msg-input",
                        lines=2,
                        max_lines=5
                    )
                    
                    with gr.Row():
                        send_button = gr.Button("üöÄ Send", variant="primary", elem_classes="primary-btn", scale=2)
                        regenerate_button = gr.Button("üîÑ Regenerate", elem_classes="secondary-btn", scale=1)
                        stop_button = gr.Button("‚èπÔ∏è Stop", elem_classes="danger-btn", scale=1)
                        clear_button = gr.Button("üóëÔ∏è Clear", elem_classes="secondary-btn", scale=1)
        
        # ‰∫ã‰ª∂ÁªëÂÆö
        enter_event = msg.submit(
            user, [msg, chatbot, state], [msg, chatbot, state], queue=True
        ).then(
            bot,
            [chatbot, temperature, top_p, top_k, remain_total, model_type, state],
            [chatbot, speed_box, compression_box, state],
        )
        send_event = send_button.click(
            user, [msg, chatbot, state], [msg, chatbot, state], queue=True
        ).then(
            bot,
            [chatbot, temperature, top_p, top_k, remain_total, model_type, state],
            [chatbot, speed_box, compression_box, state],
        )
        regenerate_event = regenerate_button.click(
            regenerate,
            [chatbot, state],
            [chatbot, msg, speed_box, compression_box, state],
            queue=True,
        ).then(
            bot,
            [chatbot, temperature, top_p, top_k, remain_total, model_type, state],
            [chatbot, speed_box, compression_box, state],
        )
        clear_button.click(
            clear, [chatbot, state], [chatbot, speed_box, compression_box, state]
        )
        stop_button.click(
            fn=None,
            inputs=None,
            outputs=None,
            cancels=[send_event, regenerate_event, enter_event],
        )
    return demo


args = parse_args()

# Set CUDA_VISIBLE_DEVICES BEFORE any CUDA operations
# This must be done before torch.cuda is initialized
if args.device and args.device.startswith("cuda:"):
    # Extract device index from user input (e.g., "cuda:1" -> "1")
    device_idx = args.device.split(":")[1]
    os.environ["CUDA_VISIBLE_DEVICES"] = device_idx
    # After setting CUDA_VISIBLE_DEVICES, the specified device becomes index 0
    # So we need to use cuda:0 instead of the original device string
    device = select_device("cuda:0")
elif args.device and args.device == "cpu":
    # User explicitly requested CPU, don't set CUDA_VISIBLE_DEVICES
    device = select_device("cpu")
elif "CUDA_VISIBLE_DEVICES" not in os.environ:
    # No device specified and no CUDA_VISIBLE_DEVICES set, default to device 0
    os.environ.setdefault("CUDA_VISIBLE_DEVICES", "0")
    device = select_device(None)
else:
    # CUDA_VISIBLE_DEVICES already set (e.g., by environment or script)
    # Use the device as specified, or auto-detect
    device = select_device(args.device)

# Load shared base model once to save memory
print("Loading shared base model...")
from transformers import AutoConfig
from dart.model.modeling_llama_kv import LlamaForCausalLM as KVLlamaForCausalLM
from dart.model.modeling_qwen3_kv import Qwen3ForCausalLM as KVQwen3ForCausalLM
from dart.model.modeling_mixtral_kv import MixtralForCausalLM as KVMixtralForCausalLM
from dart.model.modeling_qwen3_moe_kv import Qwen3MoeForCausalLM as KVQwen3MoeForCausalLM

base_model_config = AutoConfig.from_pretrained(args.base_model_name_or_path)
Type = base_model_config.architectures[0]
if Type == 'LlamaForCausalLM':
    shared_base_model = KVLlamaForCausalLM.from_pretrained(
        args.base_model_name_or_path,
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True,
        device_map="auto",
    )
elif Type == 'Qwen3ForCausalLM':
    shared_base_model = KVQwen3ForCausalLM.from_pretrained(
        args.base_model_name_or_path,
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True,
        device_map="auto",
    )
elif Type == 'Qwen3MoeForCausalLM':
    shared_base_model = KVQwen3MoeForCausalLM.from_pretrained(
        args.base_model_name_or_path,
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True,
        device_map="auto",
    )
else:
    shared_base_model = KVMixtralForCausalLM.from_pretrained(
        args.base_model_name_or_path,
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True,
        device_map="auto",
    )
shared_base_model.eval()
print("Shared base model loaded successfully.")

print("Loading Ngram model...")
ngram_model = DartModel.ngram_from_pretrained(args.ngram_model_name_or_path, args.use_small_ngram)


# Load DART model with shared base model
print("Loading DART model...")
model = DartModel.from_pretrained_with_base(
    base_model=shared_base_model,
    ngram_model=ngram_model,
    base_model_config=base_model_config,
    dart_model_name_or_path=args.dart_model_name_or_path,
    base_model_name_or_path=args.base_model_name_or_path,
).to(device)
model.eval()
print("DART model loaded successfully.")
warmup(model, args.template_name, device)

eagle_model = None
if args.compare_eagle3:
    print("Loading EAGLE model with shared base model...")
    eagle_model = EaModel.from_pretrained_with_base(
        base_model=shared_base_model,
        base_model_config=base_model_config,
        base_model_name_or_path=args.base_model_name_or_path,
        ea_model_path=args.eagle3_model_name_or_path,
        total_token=60,
        use_eagle3=True,
    )
    eagle_model.eval()
    print("EAGLE model loaded successfully.")

demo = build_demo()
demo.queue()
demo.launch(
    share=args.share,
    server_name="0.0.0.0" if args.listen else None,
    server_port=args.server_port,
)
